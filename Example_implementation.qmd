---
title: Risk sharing
description: balalalbadldb
authors:
  - name: Nicholas Harbour
format: 
  html:
    embed-resources: true
    code-fold: true
    number-sections: true
    toc: true
    toc-depth: 3
    date: now
    date-modified: last-modified
    date-format: "MMMM DD, YYYY, HH:mm:ss"
jupyter: python3
---


```{python}

import numpy as np  
from scipy.optimize import minimize
from scipy.optimize import differential_evolution
import matplotlib.pyplot as plt
import pandas as pd

```


A two-by-two model


linear risk sharing rule 




```{python}

# Data
sd = 0.1
np.random.seed(42)
X1 = np.random.normal(loc=0.1, scale=sd, size=10000)
X2 = np.random.normal(loc=0.7, scale=sd, size=10000)

mu1 = np.mean(X1)
mu2 = np.mean(X2)

# Objective with penalty for constraints
def objective_penalty(params):
    b11, b22 = params

    b = np.array([[b11, 1 - b22],
                  [1 - b11, b22]])
    
    H = b @ np.array([X1, X2])
    
    var_sum = np.var(H)
    
    # Penalize mean constraint violations heavily
    penalty = 1e6 * (abs(np.mean(H[0,:]) - mu1) + abs(np.mean(H[1,:]) - mu2))
    
    return var_sum + penalty

# Bounds for b11, b22
bounds = [(0, 1), (0, 1)]

# Run differential evolution
result = differential_evolution(objective_penalty, bounds, strategy='best1bin', tol=1e-8)

print("Optimal parameters from differential evolution:", result.x)
print("Objective value:", result.fun)

```


We can varify this correct by using the formula

$$
B^* = \frac{1}{n} \vec{1} \vec{1}^T + \frac{1}{\mu^T_X \Sigma^{-1} \mu_X} (1 - \frac{1}{n} \vec{1}\vec{1}^T) \mu_X \mu_X^T \Sigma^{-1}
$$

Where $\mu_X$ is the vector of means and $\Sigma$ is the covariance matrix of the losses.

$$
\Sigma = \begin{pmatrix}
\text{Var}(X_1) & 0 \\
0 & \text{Var}(X_2)
\end{pmatrix}
$$



```{python}


means = np.array([np.mean(X1), np.mean(X2)])
Sigma = np.diag([np.var(X1), np.var(X2)])  # Covariance matrix
# Calculate the optimal parameters using the formula
n = 2  # Number of agents
mu_X = means.reshape(-1, 1)
Sigma_inv = np.linalg.inv(Sigma)
M = mu_X.T @ Sigma_inv @ mu_X
B_star = (1/n) * np.ones((n,n)) + (1/M) * (np.eye(n) - (1/n) * np.ones((n,n))) @ mu_X @ mu_X.T @ Sigma_inv
print("Optimal parameters from formula:\n", B_star[0, 0], B_star[1, 1])
print("Optimal parameters from optimization:\n", result.x)



```
